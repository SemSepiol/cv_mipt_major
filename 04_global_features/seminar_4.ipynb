{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар №4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:37:07.531123Z",
     "start_time": "2020-10-17T17:37:06.625090Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import skimage.exposure\n",
    "import skimage.io\n",
    "import skimage.feature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:37:07.543126Z",
     "start_time": "2020-10-17T17:37:07.537126Z"
    }
   },
   "outputs": [],
   "source": [
    "# вспомогательная функция\n",
    "def plot_transform_result(src_image, transform_image, is_gray=False):\n",
    "    \"\"\"\n",
    "    Отрисовать с помощью plt исходное изображение и его преобразование.\n",
    "    \n",
    "    :param src_image: np.ndarray: исходное изображение\n",
    "    :param transform_image: np.ndarray: преобразованное изображение\n",
    "    :param is_gray: bool: флаг для отображения ЧБ изображений\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, m_axs = plt.subplots(1, 2, figsize=(6.4*2, 4.8*2), constrained_layout=True)\n",
    "    ax1, ax2 = m_axs\n",
    "\n",
    "    cmap = 'gray' if is_gray else None\n",
    "    ax1.set_title('Исходное изображение')\n",
    "    ax1.imshow(src_image, cmap=cmap)\n",
    "    ax1.set_xticks([]), ax1.set_yticks([])\n",
    "    ax2.set_title('Результат преобразования')\n",
    "    ax2.imshow(transform_image, cmap=cmap)\n",
    "    ax2.set_xticks([]), ax2.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:37:08.287902Z",
     "start_time": "2020-10-17T17:37:08.283901Z"
    }
   },
   "outputs": [],
   "source": [
    "# вспомогательная функция\n",
    "def plot_one_image(src_image, is_gray=False):\n",
    "    \"\"\"\n",
    "    Отрисовать с помощью plt исходное изображение.\n",
    "    \n",
    "    :param src_image: np.ndarray: исходное изображение\n",
    "    :param is_gray: bool: флаг для отображения ЧБ изображений\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig, m_axs = plt.subplots(1, 1, figsize=(6.4*2, 4.8*2), constrained_layout=True)\n",
    "    ax1 = m_axs\n",
    "\n",
    "    cmap = 'gray' if is_gray else None\n",
    "    ax1.set_title('Исходное изображение')\n",
    "    ax1.imshow(src_image, cmap=cmap)\n",
    "    ax1.set_xticks([]), ax1.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Матрицы смежности. Gray-Level Co-Occurrence Matrix (GLCM)\n",
    " \n",
    "Давайте искать неоднородности и разделять текстуры!\n",
    "\n",
    "<img src=\"https://i.ibb.co/y6FMPLX/image--022.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "Матрица совпадений – это статистический метод исследования текстуры изображения в градациях серого. Рассмотрии $I(k,k)$ изображение в градациях серого относительно центрального пикселя $(n_c, m_c)$. Значения совпадения определяется как распределение значений совпадений на данном расстоянии от определенного пикселя $(n_c, m_c)$. Для изображения $I(k,k)$ матрица совпадений $C_M=C_{(D_x, D_y)}(k,k) $определяется как:\n",
    "\n",
    "$C_M = \\sum_{k}^{n=1}\\sum_{k}^{m=1} \\begin{cases} 1, if \\ I(n,m)=k \\ and \\ I(n+D_x, m+D_y)=k \\\\ 0, \\ otherwise \\end{cases}$\n",
    "\n",
    "где $D_x$, $D_y$ параметр сдвига, задающий взаимное расположение пикселей и определятся следующим образом:\n",
    "\n",
    "$D_x = D cos(\\theta)$, $D_y = D sin(\\theta)$, а $\\theta$ – смещение, которое определяет направление матрицы от центрального пикселя и расстояние от центрального пикселя, как показано на рисунке\n",
    "\n",
    "$I(n,m)$ – уровень яркости пикселя изображения, расположенного в точке $(n, m)$.\n",
    "\n",
    "<img src=\"img/co-matrix_expl.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матрицы смежности: характиристики\n",
    "\n",
    "1. $Contrast = \\sum_{i,j=0}^{levels-1} C_{i,j}(i-j)^2$\n",
    "\n",
    "Контраст является мерой локального изменения интенсивности, отдавая предпочтение значениям от диагонали (i = j). Чем больше значение, тем больше различие в значениях интенсивности среди соседних пикселей.\n",
    "\n",
    "2. $Dissimilarity = \\sum_{i,j=0}^{levels-1}C_{i,j}|i-j|$\n",
    "\n",
    "Отражает меру несимметричности элементов в GLCM\n",
    "\n",
    "3. $Homogeneity = \\sum_{i,j=0}^{levels-1}\\frac{C_{i,j}}{1+(i-j)^2}$\n",
    "\n",
    "Отражает близость распределения элементов в GLCM\n",
    "\n",
    "4. $Energy = \\sum_{i,j=0}^{levels-1} C_{i,j}^2$\n",
    "\n",
    "Энергия является мерой однородных структур на изображении. Большая энергия подразумевает, что в изображении появляется больше пар значений интенсивности, которые соседствуют друг с другом на более высоких частотах.\n",
    "\n",
    "5. $Aautocorrelation = \\sum_{i,j=0}^{levels-1} C_{i,j}ij$\n",
    "\n",
    "Автокорреляция является мерой величины тонкости и грубости текстуры.\n",
    "\n",
    "6. $Entropy = - \\sum_{i,j=0}^{levels-1}C_{i,j}log_{2}C_{i,j}$ \n",
    "\n",
    "Совместная энтропия - это мера случайности / изменчивости значений интенсивности соседства.\n",
    "\n",
    "7. $Difference \\ Entropy = \\sum_{k=0}^{levels-2} C_{x-y}(k)log_{2}(C_{x-y}(k) + \\epsilon$\n",
    "\n",
    "Разностная Энтропия является мерой случайности / изменчивости в разнице значений интенсивности окрестности.\n",
    "\n",
    "8. $Correlation = \\sum_{i,j=0}^{levels-1} C_{i,j}\\left[\\frac{(i-\\mu_i) \\\n",
    "(j-\\mu_j)}{\\sqrt{(\\sigma_i^2)(\\sigma_j^2)}}\\right]$, где\n",
    "\n",
    "    $\\mu_i = \\sum_{i=1}^{L}\\sum_{j=1}^{L}iC_M$ \n",
    "\n",
    "    $\\mu_j = \\sum_{i=1}^{L}\\sum_{j=1}^{L}jC_M$, \n",
    "\n",
    "    $\\sigma_{i}^2 = \\sum_{i=1}^{L}\\sum_{j=1}^{L}C_M(i-\\mu_{i}^2)$\n",
    "\n",
    "    $\\sigma_{j}^2 = \\sum_{i=1}^{L}\\sum_{j=1}^{L}C_M(j-\\mu_{j}^2)$\n",
    "    \n",
    "    Корреляция - это значение между 0 (некоррелированный) и 1 (идеально коррелированный), показывающее линейную зависимость значений уровня серого от их соответствующих пикселей в GLCM.\n",
    "    \n",
    "9. $Cluster \\ Prominence = \\sum_{i,j=0}^{levels-1} (i + j - \\mu_{x} - \\mu_{y})^4 C_{i,j}$\n",
    "\n",
    "Распространенность кластеров - это мера асимметрии и асимметрии GLCM. Более высокие значения подразумевают большую асимметрию относительно среднего, в то время как более низкое значение указывает на пик около среднего значения и меньшую вариацию относительно среднего.\n",
    "\n",
    "10. $Cluster \\ Shade = \\sum_{i,j=0}^{levels-1} (i + j - \\mu_{x} - \\mu_{y})^3 C_{i,j}$\n",
    "\n",
    "Кластерный оттенок - это показатель асимметрии и однородности GLCM. Более высокий оттенок кластера подразумевает большую асимметрию относительно среднего значения.\n",
    "\n",
    "11. $Cluster \\ Tendency = \\sum_{i,j=0}^{levels-1} (i + j - \\mu_{x} - \\mu_{y})^2 C_{i,j} $\n",
    "\n",
    "Кластерная Тенденция - это мера группирования вокселей с одинаковыми значениями уровня серого.\n",
    "\n",
    "12. $Joint \\ Average = \\mu_{x} = \\sum_{i,j=0}^{levels-1} C_{i,j}i$\n",
    "\n",
    "Совместное Среднее - средняя интенсивность уровня серого распределения i.\n",
    "\n",
    "13. $Difference \\ Average = \\sum_{k=0}^{levels-2} kC_{x-y}(k)$\n",
    "\n",
    "$ C_{x-y}(k) = \\sum_{i,j=0}^{levels-1} C_{i,j}$, где $|i-j| = k$ и $k = 0, 1, ..., levels - 2$.\n",
    "\n",
    "Среднее различие измеряет взаимосвязь между появлением пар с одинаковыми значениями интенсивности и появлением пар с различными значениями интенсивности.\n",
    "\n",
    "14. $Difference \\ Variance = \\sum_{k=0}^{levels-2} (k-DA)^2 C_{x-y}(k)$\n",
    "\n",
    "Разница является мерой неоднородности, которая помещает более высокие веса в пары различных уровней интенсивности, которые больше отклоняются от среднего.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:37:44.692400Z",
     "start_time": "2020-10-17T17:37:44.003945Z"
    }
   },
   "outputs": [],
   "source": [
    "carpet = cv2.imread('img/carpet_ex.jpg')\n",
    "carpet = cv2.cvtColor(carpet, cv2.COLOR_BGR2RGB)\n",
    "forest = cv2.imread('img/forest_ex.jpg')\n",
    "forest = cv2.cvtColor(forest, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plot_one_image(carpet)\n",
    "plot_one_image(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:44:01.785739Z",
     "start_time": "2020-10-17T17:44:00.756496Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.feature import greycomatrix\n",
    "\n",
    "gray_carpet = cv2.cvtColor(carpet, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "glcm = greycomatrix(gray_carpet, [2, 16], [0, np.pi/2],\n",
    "                    normed=False, symmetric=False, levels=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:46:03.469605Z",
     "start_time": "2020-10-17T17:46:03.462568Z"
    }
   },
   "outputs": [],
   "source": [
    "glcm.shape, carpet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:46:03.986835Z",
     "start_time": "2020-10-17T17:46:03.980801Z"
    }
   },
   "outputs": [],
   "source": [
    "gray_carpet[:4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:46:06.103033Z",
     "start_time": "2020-10-17T17:46:06.098029Z"
    }
   },
   "outputs": [],
   "source": [
    "glcm[:4, :4, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:46:11.340798Z",
     "start_time": "2020-10-17T17:46:11.334817Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glcm[:4, :4, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые характеристики реализваоны в ```skimage.feature.greycoprops```:\n",
    "- *Contrast*\n",
    "\n",
    "- *Dssimilarity*\n",
    "\n",
    "- *Hmogeneity*\n",
    "\n",
    "- *ASM*\n",
    "\n",
    "- *Energy*\n",
    "\n",
    "- *correlation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:48:37.400014Z",
     "start_time": "2020-10-17T17:48:37.378463Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.feature import greycoprops\n",
    "\n",
    "feature = greycoprops(glcm, prop='correlation')\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:49:12.140214Z",
     "start_time": "2020-10-17T17:49:12.106177Z"
    }
   },
   "outputs": [],
   "source": [
    "gray_forest = cv2.cvtColor(forest, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "glcm = greycomatrix(gray_forest, [2, 16], [0, np.pi/2],\n",
    "                    normed=False, symmetric=False, levels=256)\n",
    "\n",
    "feature = greycoprops(glcm, prop='correlation')\n",
    "feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контуры на изображении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оператор Собеля\n",
    "\n",
    "**Оператор Собеля** $-$ это дискретный дифференциальный оператор, вычисляющий приближение градиента яркости изображения.\n",
    "Оператор вычисляет градиент яркости изображения в каждой точке. Так находится направление наибольшего увеличения яркости и величина её изменения в этом направлении. Результат показывает, насколько «резко» или «плавно» меняется яркость изображения в каждой точке, а значит, вероятность нахождения точки на грани, а также ориентацию границы.\n",
    "\n",
    "Т.о. результатом работы оператора Собеля в точке области постоянной яркости будет нулевой вектор, а в точке, лежащей на границе областей различной яркости — вектор, пересекающий границу в направлении увеличения яркости.\n",
    "\n",
    "Наиболее часто оператор Собеля применяется в алгоритмах выделения границ. \n",
    "\n",
    "Оператор Собеля основан на свёртке изображения небольшими целочисленными фильтрами в вертикальном и горизонтальном направлениях, поэтому его относительно легко вычислять. Оператор использует ядра 3x3, с которыми свёртывают исходное изображение для вычисления приближенных значений производных по горизонтали и по вертикали.\n",
    "\n",
    "\n",
    "### Формализация\n",
    "Пусть ${\\displaystyle \\mathbf {A} }$ $-$ это исходное изображение, а ${\\displaystyle \\mathbf {G} _{x}}$ и ${\\displaystyle \\mathbf {G} _{y}}$ $-$ два изображения, на которых каждая точка содержит приближённые производные по ${\\displaystyle x}$ и по ${\\displaystyle y}$. Они вычисляются следующим образом:\n",
    "\n",
    "${\\displaystyle \\mathbf {G} _{y}={\\begin{bmatrix}-1&-2&-1\\\\0&0&0\\\\+1&+2&+1\\\\\\end{bmatrix}}*\\mathbf {A} \\quad {\\mbox{and}}\\quad \\mathbf {G} _{x}={\\begin{bmatrix}-1&0&+1\\\\-2&0&+2\\\\-1&0&+1\\end{bmatrix}}*\\mathbf {A} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контур и как его найти"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Контурный анализ** $-$ это один из важных и очень полезных методов описания, хранения, распознавания, сравнения и поиска графических образов/объектов. \n",
    "\n",
    "**Контур** $-$ это внешние очертания (обвод) предмета/объекта.\n",
    "\n",
    "При проведении контурного анализа:\n",
    "* полагается, что контур содержит достаточную информацию о форме объекта;\n",
    "* внутренние точки объекта во внимание не принимаются. \n",
    "\n",
    "Вышеприведённые положения, разумеется, накладывают существенные ограничения на область применения контурного анализа, которые, в основном, связаны с проблемами выделения контура на изображениях:\n",
    "* из-за одинаковой яркости с фоном объект может не иметь чёткой границы, или может быть зашумлён помехами, что приводит к невозможности выделения контура;\n",
    "* перекрытие объектов или их группировка приводит к тому, что контур выделяется неправильно и не соответствует границе объекта.\n",
    "\n",
    "Однако, переход к рассмотрению только контуров объектов позволяет уйти от пространства изображения – к пространству контуров, что существенно снижает сложность алгоритмов и вычислений. \n",
    "\n",
    "Т.о., контурный анализ имеет довольно слабую устойчивость к помехам, и любое пересечение или лишь частичная видимость объекта приводит либо к невозможности детектирования, либо к ложным срабатываниям, но простота и быстродействие контурного анализа, позволяют вполне успешно применять данный подход (при чётко выраженном объекте на контрастном фоне и отсутствии помех).\n",
    "\n",
    "Итак, мы определились, что контур — это некая граница объекта, которая отделяет его от фона (других объектов). \n",
    "\n",
    "Во всех случаях мы получаем бинарное изображение, которое явным образом задаёт нам границы объекта. Вот эта совокупность пикселей, составляющих границу объекта и есть контур объекта.\n",
    "\n",
    "Чтобы оперировать полученным контуром, его необходимо как-то представить (закодировать). \n",
    "Например, указывать вершины отрезков, составляющих контур.\n",
    "Другой известный способ кодирования контура $-$ это **цепной код Фримена**. Этот метод будет рассмотрен чуть позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оператор Лапласа\n",
    "Он вычисляет лапласиан изображения, заданного соотношением,\n",
    "${\\Delta src = \\frac{\\partial ^2{src}}{\\partial x^2} + \\frac{\\partial ^2{src}}{\\partial y^2}}$\n",
    "где каждая производная находится с использованием производных Собеля. Если ksize = $3$, то для фильтрации используется следующее ядро:\n",
    "\n",
    "$$\n",
    "{K = \\begin{pmatrix}\n",
    "0 & \\ 1 & \\ 0 \\\\ \n",
    "1 & \\ -4 & \\ 1 \\\\ \n",
    "0 & \\ 1 & \\ 0 \n",
    "\\end{pmatrix}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:54:00.650985Z",
     "start_time": "2020-10-17T17:54:00.299689Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/lk.jpg')\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "## выделяем границы\n",
    "gray_img = cv2.GaussianBlur(gray_img, ksize=(7, 7), sigmaX=1, sigmaY=1)\n",
    "laplac = cv2.Laplacian(gray_img, cv2.THRESH_BINARY, scale=0.55, ksize=5)\n",
    "\n",
    "plot_transform_result(gray_img, laplac, is_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектор границ Кенни (Canny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теория\n",
    "**Canny Edge Detection** $-$ популярный алгоритм обнаружения краев. Это многоступенчатый алгоритм, и мы пройдем через все этапы.\n",
    "\n",
    "1. **Шумоподавление**\n",
    "\n",
    "Поскольку обнаружение краев подвержено воздействию шума на изображении, первым шагом является удаление шума на изображении с помощью фильтра Гаусса $5\\times5$. Мы уже видели это в предыдущих главах.\n",
    "\n",
    "2. **Поиск градиента интенсивности изображения**\n",
    "\n",
    "Затем сглаженное изображение фильтруется ядром Собеля (рассмотрен выше) в горизонтальном и вертикальном направлении, чтобы получить первую производную в горизонтальном направлении ($G_x$) и вертикальном направлении ($G_y$). Из этих двух изображений мы можем найти градиент края и направление для каждого пикселя следующим образом:\n",
    "\n",
    "$${Edge(G) = \\sqrt{ G_x^2 + G_y^2}}$$\n",
    "$${Angle(\\theta) = \\tan^{-1}(\\frac{G_x}{G_y})}$$\n",
    "\n",
    "Направление градиента всегда перпендикулярно краям. Он округлен до одного из четырех углов, представляющих вертикальное, горизонтальное и два диагональных направления.\n",
    "\n",
    "3. **Немаксимальное подавление**\n",
    "\n",
    "После получения величины и направления градиента выполняется полное сканирование изображения для удаления любых нежелательных пикселей, которые могут не составлять края. Для этого в каждом пикселе пиксель проверяется, является ли он локальным максимумом в его окрестности в направлении градиента. Проверьте изображение ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/XZZVNmK/nms.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "Точка $А$ находится на краю (в вертикальном направлении). Направление градиента нормальное к краю. Точки $B$ и $C$ находятся в градиентных направлениях. Таким образом, точка $A$ проверяется с помощью точек $B$ и $C$, чтобы увидеть, образует ли она локальный максимум. Если это так, он рассматривается для следующего этапа, в противном случае он подавляется (обнуляется).\n",
    "\n",
    "Короче говоря, в результате вы получите бинарное изображение с «тонкими краями».\n",
    "\n",
    "4. **Подбор порогового значения**\n",
    "\n",
    "Эта стадия решает, какие ребра действительно являются ребрами, а какие нет. Для этого нам понадобятся два пороговых значения, **minVal** и **maxVal**. Любые ребра с градиентом интенсивности, превышающим **maxVal**, обязательно будут ребрами, а ребра ниже **minVal** не будут ребрами, поэтому отбрасываются. Те, кто лежит между этими двумя порогами, классифицируются как ребра или не ребра в зависимости от их связности. Если они связаны с точными пикселями, они считаются частью ребер. В противном случае они также отбрасываются. Смотрите изображение ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/cryRqvD/hysteresis.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "Край $A$ выше **maxVal**, так что считается «верным краем». Хотя ребро $C$ меньше **maxVal**, оно связано с ребром $A$, так что это также считается допустимым ребром, и мы получаем эту полную кривую. Но ребро $B$, хотя оно выше **minVal** и находится в той же области, что и ребро $C$, не связано с каким-либо «верным краем», поэтому отбрасывается. Поэтому очень важно, чтобы мы выбрали соответственно **minVal** и **maxVal**, чтобы получить правильный результат.\n",
    "\n",
    "На этом этапе также удаляются небольшие пиксельные шумы в предположении, что края являются длинными линиями.\n",
    "\n",
    "Итак, что мы в итоге получаем, это сильные края изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектор границ Кенни в OpenCV\n",
    "OpenCV помещает все вышеперечисленное в одну функцию ```cv2.Canny(image,threshold1,threshold2,apertureSize,L2gradient)```. \n",
    "\n",
    "* **image** $-$ это наше входное изображение\n",
    "* **threshold1** $-$ minVal для процедуры гистерезиса\n",
    "* **threshold2** $-$ maxVal для процедуры гистерезиса\n",
    "* **apertureSize** $-$ размер ядра Собеля, используемый для поиска градиентов изображения, по умолчанию равен $3$\n",
    "* **L2gradient** $-$ флаг, определяет уравнение для определения величины градиента. Если это True, он использует упомянутое выше уравнение, которое является более точным, в противном случае он использует эту функцию: Edge_Gradient($G$) = |$G_x$| + |$G_y$|. По умолчанию это False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:56:41.218264Z",
     "start_time": "2020-10-17T17:56:41.214300Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_canny(image, sigma=0.33):\n",
    "    # compute the median of the single channel pixel intensities\n",
    "    v = np.median(image)\n",
    "\n",
    "    # apply automatic Canny edge detection using the computed median\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    edged = cv2.Canny(image, lower, upper)\n",
    "\n",
    "    # return the edged image\n",
    "    return edged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('img/lk.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:56:42.308740Z",
     "start_time": "2020-10-17T17:56:41.974954Z"
    }
   },
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "edges = auto_canny(blurred)\n",
    "\n",
    "plot_transform_result(gray, edges, is_gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T17:58:38.834696Z",
     "start_time": "2020-10-17T17:58:38.494486Z"
    }
   },
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "edges = cv2.Laplacian(blurred, cv2.THRESH_BINARY, scale=0.25, ksize=7)\n",
    "\n",
    "plot_transform_result(gray, edges, is_gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = skimage.feature.canny(gray, sigma=1)\n",
    "\n",
    "plot_transform_result(gray, edges, is_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выделяем контуры\n",
    "\n",
    "Полученные границы достаточно просто преобразуются в контуры. Для алгоритма Кэнни это происходит автоматически, для остальных алгоритмов требуется дополнительная бинаризация. Получить контур для бинарного алгоритма можно например алгоритмом [жука](http://wiki.technicalvision.ru/index.php/%D0%92%D1%8B%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B8_%D0%BE%D0%BF%D0%B8%D1%81%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BA%D0%BE%D0%BD%D1%82%D1%83%D1%80%D0%BE%D0%B2).\n",
    "\n",
    "В OpenCV поиск контуров похож на поиск белого объекта на черном фоне. Помните, что объект, который нужно найти, должен быть белым, а фон должен быть черным.\n",
    "Давайте посмотрим, как найти контуры двоичного изображения с помощью метода **Фридмана**:\n",
    "\n",
    "**Цепной код Фримена (Фридмана) (Freeman Chain Code)**\n",
    "\n",
    "Цепные коды применяются для представления границы в виде последовательности отрезков прямых линий определённой длины и направления. В основе этого представления лежит 4- или 8- связная решётка. Длина каждого отрезка определяется разрешением решётки, а направления задаются выбранным кодом.\n",
    "(для представления всех направлений в 4-связной решётке достаточно 2-х бит, а для 8-связной решётки цепного кода требуется 3 бита)\n",
    "\n",
    "<img src=\"https://i.ibb.co/6tyGLKS/freeman_chain_code.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "Если честно, то у меня ни разу ни получилось применить контурный анализ в реальных задачах. Уж слишком идеальные условия требуются. То граница не найдётся, то шумов слишком много. Но, если нужно что-то распознавать в идеальных условиях $-$ то контурный анализ замечательный вариант. Очень быстро работает, красивая математика и понятная логика."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В OpenCV для поиска контуров имеется функцией findContours, которая имеет вид:\n",
    "\n",
    "```findContours(img, hierarchy, mode, method, offset)```\n",
    "\n",
    "\n",
    "1. img — должным образом подготовленная для анализа картинка. Это должно быть 8-битное изображение. Поиск контуров использует для работы монохромное изображение, так что все пиксели картинки с ненулевым цветом будут интерпретироваться как 1, а все нулевые останутся нулями.\n",
    "\n",
    "\n",
    "2. mode — один из четырех режимов группировки найденных контуров:\n",
    "    * CV_RETR_LIST — выдаёт все контуры без группировки;\n",
    "    * CV_RETR_EXTERNAL — выдаёт только крайние внешние контуры. Например, если в кадре будет пончик, то функция вернет его внешнюю границу без дырки.\n",
    "    * CV_RETR_CCOMP — группирует контуры в двухуровневую иерархию. На верхнем уровне — внешние контуры объекта. На втором уровне — контуры отверстий, если таковые имеются. Все остальные контуры попадают на верхний уровень.\n",
    "    * CV_RETR_TREE — группирует контуры в многоуровневую иерархию.\n",
    "\n",
    "\n",
    "3. method — один из трёх методов упаковки контуров:\n",
    "    * CV_CHAIN_APPROX_NONE — упаковка отсутствует и все контуры хранятся в виде отрезков, состоящих из двух пикселей.\n",
    "    * CV_CHAIN_APPROX_SIMPLE — склеивает все горизонтальные, вертикальные и диагональные контуры.\n",
    "    * CV_CHAIN_APPROX_TC89_L1,CV_CHAIN_APPROX_TC89_KCOS — применяет к контурам метод упаковки (аппроксимации) Teh-Chin.\n",
    "\n",
    "\n",
    "4. hierarchy — список всех найденных контуров, представленных в виде векторов; иерархия — информация о топологии контуров. Каждый элемент иерархии представляет собой сборку из четырех индексов, которая соответствует контуру[i]:\n",
    "    * иерархия[i][0] — индекс следующего контура на текущем слое;\n",
    "    * иерархия[i][1] — индекс предыдущего контура на текущем слое:\n",
    "    * иерархия[i][2] — индекс первого контура на вложенном слое;\n",
    "    * иерархия[i][3] — индекс родительского контура.\n",
    "\n",
    "\n",
    "5. offset — величина смещения точек контура. Это полезно, если контуры извлекаются из ROI, а затем они должны анализироваться во всем контексте изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:07:03.909818Z",
     "start_time": "2020-10-17T18:07:03.707749Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "blur = cv2.medianBlur(gray_img, 3)\n",
    "laplac = cv2.Laplacian(blur, cv2.THRESH_BINARY, scale=0.15, ksize=5)\n",
    "\n",
    "contours, hierarchy = cv2.findContours(laplac, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "plot_transform_result(gray, laplac, is_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как нарисовать контуры?\n",
    "\n",
    "Для рисования контуров используется функция:\n",
    "\n",
    "```drawContours(image, contours, contourIdx, color, thickness, lineType, hierarchy, maxLevel, offset)```\n",
    "\n",
    "1. image — кадр, поверх которого мы будем отрисовывать контуры; \n",
    "\n",
    "\n",
    "2. contours — те самые контуры, найденные функцией findContours; \n",
    "\n",
    "\n",
    "3. contourIdx — индекс контура, который следует отобразить. \n",
    "    * -1 — если нужно отобразить все контуры; \n",
    "\n",
    "\n",
    "4. color — цвет контура; \n",
    "\n",
    "\n",
    "5. thickness — толщина линии контура; \n",
    "\n",
    "\n",
    "6. lineType — тип соединения точек вектора; \n",
    "\n",
    "\n",
    "7. hierarchy — информация об иерархии контуров; \n",
    "\n",
    "\n",
    "8. maxLevel — индекс слоя, который следует отображать. \n",
    "    * Если параметр равен 0, то будет отображен только выбранный контур. Если параметр равен 1, то отобразится выбранный контур и все его дочерние контуры. Если параметр равен 2, то отобразится выбранный контур, все его дочерние и дочерние дочерних! И так далее. \n",
    "    \n",
    "\n",
    "9. offset — величина смещения точек контура.\n",
    "\n",
    "Его также можно использовать для рисования любой фигуры, если у вас есть граничные точки. Его первый аргумент является исходным изображением, второй аргумент - это контуры, которые должны быть переданы в виде списка Python, третий аргумент - это индекс контуров (полезно при рисовании отдельного контура. Чтобы нарисовать все контуры, передайте -1), а остальные аргументы - это цвет, толщина и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:12:37.071453Z",
     "start_time": "2020-10-17T18:12:37.068489Z"
    }
   },
   "outputs": [],
   "source": [
    "new_contours = []\n",
    "for cnt in contours:\n",
    "    if cnt.shape[0] > 30:\n",
    "        new_contours.append(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:12:37.343714Z",
     "start_time": "2020-10-17T18:12:37.336403Z"
    }
   },
   "outputs": [],
   "source": [
    "len(contours) - len(new_contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:12:37.796967Z",
     "start_time": "2020-10-17T18:12:37.555965Z"
    }
   },
   "outputs": [],
   "source": [
    "# нарисуем все найденные контуры\n",
    "img1 = gray_img.copy()\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img1, new_contours, 15, (255, 0, 0), 3)\n",
    "\n",
    "plot_one_image(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:12:44.786671Z",
     "start_time": "2020-10-17T18:12:44.541590Z"
    }
   },
   "outputs": [],
   "source": [
    "# нарисуем один выбранный контур\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img2, contours, 15, (255, 0, 0), 2)\n",
    "\n",
    "plot_one_image(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ключевые признаки контура\n",
    "\n",
    "Пройдемся по основным методам работы с характеристиками конутра, которые доступны в OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Моменты\n",
    "Моменты изображения помогают вам рассчитать некоторые функции, такие как центр масс объекта, площадь объекта и т. д.\n",
    "\n",
    "Функция **cv2.moments()** предоставляет словарь всех вычисленных значений моментов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Центральные моменты__\n",
    "\n",
    "$\\mu _{{pq}}=\\sum _{{x}}\\sum _{{y}}(x-{\\bar  {x}})^{p}(y-{\\bar  {y}})^{q}f(x,y)$\n",
    "\n",
    "__Масштабные инварианты__\n",
    "\n",
    "Инварианты $η_{ij}$ относительно сдвига и масштаба могут быть построены из центральных моментов путем деления на правильно масштабированный нулевой центральный момент:\n",
    "\n",
    "${\\displaystyle \\eta _{ij}={\\frac {\\mu _{ij}}{\\mu _{00}^{\\left(1+{\\frac {i+j}{2}}\\right)}}}\\}\\$, \n",
    "где i + j ≥ 2. Обратите внимание, что трансляционная инвариантность непосредственно следует только за счет использования центральных моментов.\n",
    "\n",
    "__Вращающиеся инварианты__\n",
    "\n",
    "Как показано в работе Ху, могут быть построены инварианты относительно перемещения, масштаба и вращения:\n",
    "\n",
    "${\\displaystyle I_{1}=\\eta _{20}+\\eta _{02}} I_{1}=\\eta _{{20}}+\\eta _{{02}}$\n",
    "\n",
    "${\\displaystyle I_{2}=(\\eta _{20}-\\eta _{02})^{2}+4\\eta _{11}^{2}} I_{2}=(\\eta _{{20}}-\\eta _{{02}})^{2}+4\\eta _{{11}}^{2}$\n",
    "\n",
    "${\\displaystyle I_{3}=(\\eta _{30}-3\\eta _{12})^{2}+(3\\eta _{21}-\\eta _{03})^{2}} I_{3}=(\\eta _{{30}}-3\\eta _{{12}})^{2}+(3\\eta _{{21}}-\\eta _{{03}})^{2}$\n",
    "\n",
    "${\\displaystyle I_{4}=(\\eta _{30}+\\eta _{12})^{2}+(\\eta _{21}+\\eta _{03})^{2}} I_{4}=(\\eta _{{30}}+\\eta _{{12}})^{2}+(\\eta _{{21}}+\\eta _{{03}})^{2}$\n",
    "\n",
    "${\\displaystyle I_{5}=(\\eta _{30}-3\\eta _{12})(\\eta _{30}+\\eta _{12})[(\\eta _{30}+\\eta _{12})^{2}-3(\\eta _{21}+\\eta _{03})^{2}]+(3\\eta _{21}-\\eta _{03})(\\eta _{21}+\\eta _{03})[3(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}]}$\n",
    "\n",
    "${\\displaystyle I_{6}=(\\eta _{20}-\\eta _{02})[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}]+4\\eta _{11}(\\eta _{30}+\\eta _{12})(\\eta _{21}+\\eta _{03})}$\n",
    "\n",
    "${\\displaystyle I_{7}=(3\\eta _{21}-\\eta _{03})(\\eta _{30}+\\eta _{12})[(\\eta _{30}+\\eta _{12})^{2}-3(\\eta _{21}+\\eta _{03})^{2}]-(\\eta _{30}-3\\eta _{12})(\\eta _{21}+\\eta _{03})[3(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}].}$\n",
    "\n",
    "${\\displaystyle I_{8}=\\eta _{11}[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{03}+\\eta _{21})^{2}]-(\\eta _{20}-\\eta _{02})(\\eta _{30}+\\eta _{12})(\\eta _{03}+\\eta _{21})}$\n",
    "\n",
    "${\\displaystyle I_{8}=\\eta _{11}[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{03}+\\eta _{21})^{2}]-(\\eta _{20}-\\eta _{02})(\\eta _{30}+\\eta _{12})(\\eta _{03}+\\eta _{21})}$\n",
    "\n",
    "Они хорошо известны как инварианты моментов Ху.\n",
    "\n",
    "Первый, I1, аналогичен моменту инерции вокруг центроида изображения, где интенсивности пикселей аналогичны физической плотности. Последний, I7, является косоинвариантным, что позволяет ему отличать зеркальные изображения от других идентичных изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:15:01.817553Z",
     "start_time": "2020-10-17T18:15:01.606987Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "blur = cv2.medianBlur(gray_img, 3)\n",
    "laplac = cv2.Laplacian(blur, cv2.THRESH_BINARY, scale=1, ksize=5)\n",
    "contours, hierarchy = cv2.findContours(laplac, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "plot_one_image(laplac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нарисуем один выбранный контур\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img2, contours, 212, (255, 0, 0), 2)\n",
    "\n",
    "plot_one_image(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:15:18.452377Z",
     "start_time": "2020-10-17T18:15:18.446458Z"
    }
   },
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:15:27.598817Z",
     "start_time": "2020-10-17T18:15:27.593820Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt = contours[212]\n",
    "M = cv2.moments(cnt)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:16:09.336086Z",
     "start_time": "2020-10-17T18:16:09.330082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Коэффициент асимметрии\n",
    "import math\n",
    "sigma_x = math.sqrt(M['m20']/M['m00'])\n",
    "sigma_y = math.sqrt(M['m02']/M['m00'])\n",
    "\n",
    "k_x = M['m30']/sigma_x**3\n",
    "k_y = M['m03']/sigma_y**3\n",
    "\n",
    "print(k_x, k_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этих моментов вы можете извлечь полезные данные, такие как площадь, центроид и т.д. Центроид определяется отношениями, ${C_x = \\frac{M_{10}}{M_{00}}}$ and ${C_y = \\frac{M_{01}}{M_{00}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:23:43.018402Z",
     "start_time": "2020-10-17T18:23:42.803413Z"
    }
   },
   "outputs": [],
   "source": [
    "Cx = int(M['m10'] /M ['m00'])\n",
    "Cy = int(M['m01'] / M['m00'])\n",
    "print('Cx =', Cx, 'Cy =', Cy)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cnt = contours[212]\n",
    "cv2.drawContours(img2, [cnt], 0, (255, 0, 0), 3)\n",
    "plt.scatter(Cx, Cy, color='blue')\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы рассчитать все моменты Ху есть функция ```cv2.HuMoments(moments)````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:23:44.600415Z",
     "start_time": "2020-10-17T18:23:44.595409Z"
    }
   },
   "outputs": [],
   "source": [
    "hu = cv2.HuMoments(M)\n",
    "\n",
    "print(hu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример сохранения моментов__\n",
    "\n",
    "<img src=\"https://i.ibb.co/1nL7q4w/HuMoments.png\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние между двумя фигурами с помощью matchShapes\n",
    "\n",
    "В этом разделе мы узнаем, как использовать моменты Ху, чтобы найти расстояние между двумя фигурами. Если расстояние маленькое, формы близки по внешнему виду, а если расстояние большое, то фигуры находятся дальше друг от друга по внешнему виду.\n",
    "\n",
    "OpenCV предоставляет простую в использовании служебную функцию matchShapes, которая берет два изображения (или контура) и находит расстояние между ними с помощью Hu Moments. Таким образом, вам не нужно явно вычислять моменты Ху. Просто оцифруйте изображения и используйте matchShapes.\n",
    "\n",
    "Использование показано ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:23:46.188531Z",
     "start_time": "2020-10-17T18:23:45.344531Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/lk.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "rows, cols = img.shape\n",
    "\n",
    "M1 = cv2.getRotationMatrix2D((cols/2, rows/2), 25, scale=1.0)\n",
    "M2 = cv2.getRotationMatrix2D((300, 700), -15, scale=0.75)\n",
    "\n",
    "dst1 = cv2.warpAffine(img.copy(), M1, (cols, rows))\n",
    "dst2 = cv2.warpAffine(img.copy(), M2, (cols, rows))\n",
    "\n",
    "plot_transform_result(img, dst1, is_gray=True)\n",
    "plot_transform_result(img, dst2, is_gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:23:46.223526Z",
     "start_time": "2020-10-17T18:23:45.868Z"
    }
   },
   "outputs": [],
   "source": [
    "d1 = cv2.matchShapes(dst1, dst2, cv2.CONTOURS_MATCH_I1,0)\n",
    "d2 = cv2.matchShapes(dst1, dst2, cv2.CONTOURS_MATCH_I2,0)\n",
    "d3 = cv2.matchShapes(dst1, dst2, cv2.CONTOURS_MATCH_I3,0)\n",
    "\n",
    "print(f'Расстояние по I1: {d1:.5f}\\n'\n",
    "      f'Расстояние по I2: {d2:.5f}\\n'\n",
    "      f'Расстояние по I3: {d3:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что есть три вида расстояний, которые можно использовать с помощью третьего параметра (CONTOURS_MATCH_I1, CONTOURS_MATCH_I2 или CONTOURS_MATCH_I3).\n",
    "\n",
    "Два изображения (im1 и im2) похожи, если указанные выше расстояния малы. Вы можете использовать любую меру расстояния. Они обычно дают похожие результаты.\n",
    "\n",
    "Давайте посмотрим, как определяются эти три расстояния.\n",
    "\n",
    "Пусть $ D (A, B) $ - расстояние между формами $ A $ и $ B $, а $ H ^ A_i $ и $ H ^ B_i $ - логарифмические преобразования $ i ^ {th} $ Ху Моментов для фигур $ A $ и $ B $. Расстояния, соответствующие трем случаям, определяются как\n",
    "\n",
    "CONTOURS_MATCH_I1\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\left | \\frac{1}{H^B_i} - \\frac{1}{H^A_i} \\right |  \\end{align*}\n",
    "\n",
    "CONTOURS_MATCH_I2\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\left | H^B_i - H^A_i \\right |  \\end{align*}\n",
    "\n",
    "CONTOURS_MATCH_I3\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\frac{\\left | H^A_i - H^B_i \\right |}{\\left | H^A_i \\right |}  \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom distance measure\n",
    "Если вы хотите определить собственную меру расстояния между двумя фигурами, вы можете легко это сделать. Например, вы можете использовать евклидово расстояние между моментами Ху, заданными\n",
    "\n",
    "  \\begin{align*} D(A, B) = \\sqrt { \\sum^{6}_{i=0} \\left ( H^B_i - H^A_i \\right )^2 } \\end{align*}\n",
    "\n",
    "Сначала вы вычисляете трансформированные в журнал моменты Ху, как упомянуто в предыдущем разделе, а затем сами вычисляете расстояние вместо использования matchShapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контурная зона\n",
    "Площадь контура задается функцией **cv2.contourArea()** или из моментов, **M['m00']**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:23:47.847847Z",
     "start_time": "2020-10-17T18:23:47.842793Z"
    }
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Периметр контура\n",
    "Это также называется длиной дуги. Это можно узнать с помощью функции **cv2.arcLength()**. Второй аргумент указывает, является ли фигура замкнутым контуром (если передан True) или просто кривой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:23:48.952363Z",
     "start_time": "2020-10-17T18:23:48.948360Z"
    }
   },
   "outputs": [],
   "source": [
    "perimeter = cv2.arcLength(cnt, True)\n",
    "print(round(perimeter, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контурное приближение\n",
    "Он приближает форму контура к другой форме с меньшим количеством вершин в зависимости от заданной нами точности. Это реализация алгоритма Дугласа-Пекера. Проверьте страницу википедии на алгоритм и демонстрацию.\n",
    "\n",
    "Чтобы понять это, предположим, что вы пытаетесь найти квадрат на изображении, но из-за некоторых проблем на изображении вы получили не идеальный квадрат, а \"плохую форму\" (как показано на первом изображении ниже). Теперь вы можете использовать эту функцию для аппроксимации формы. В этом случае второй аргумент называется эпсилон, который является максимальным расстоянием от контура до приближенного контура. Это параметр точности. Для правильного вывода необходим мудрый выбор эпсилона.\n",
    "\n",
    "<img src=\"https://i.ibb.co/jHvc2HS/approx.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "Выше, на втором изображении, зеленая линия показывает приблизительную кривую для эпсилона = $10$% длины дуги. Третье изображение показывает то же самое для эпсилона = $1$% длины дуги. Третий аргумент указывает, является ли кривая замкнутой или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:24:15.110546Z",
     "start_time": "2020-10-17T18:24:14.932023Z"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1 * cv2.arcLength(cnt,True)\n",
    "approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img2, [approx], 0, (255, 0, 0), 3)\n",
    "\n",
    "plot_one_image(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выпуклость контура\n",
    "Выпуклая оболочка будет похожа на контурную аппроксимацию, но это не так (оба могут давать одинаковые результаты в некоторых случаях). Здесь функция **cv2.convexHull()** проверяет кривую на наличие дефектов выпуклости и исправляет ее. Вообще говоря, выпуклые кривые $-$ это кривые, которые всегда выпуклые или, по крайней мере, плоские. И если он выпуклый внутри, это называется дефектами выпуклости. Например, проверьте изображение ниже. Красная линия показывает выпуклый корпус руки. Двусторонние стрелки показывают дефекты выпуклости, которые представляют собой локальные максимальные отклонения корпуса от контуров.\n",
    "<img src=\"https://i.ibb.co/Z2nPDCM/convexitydefects.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "**hull = cv2.convexHull(points, clockwise, returnPoints)**\n",
    "\n",
    "* **points** $-$ точки контура.\n",
    "\n",
    "* **clockwise** $-$ флаг ориентации. Если это правда, выходной выпуклый корпус ориентирован по часовой стрелке. В противном случае он ориентирован против часовой стрелки.\n",
    "\n",
    "* **returnPoints** $-$ по умолчанию True. Затем он возвращает координаты точек корпуса. Если False, он возвращает индексы точек контура, соответствующие точкам корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:24:29.866920Z",
     "start_time": "2020-10-17T18:24:29.661916Z"
    }
   },
   "outputs": [],
   "source": [
    "hull = cv2.convexHull(cnt)\n",
    "hull_id = cv2.convexHull(cnt, returnPoints=False)\n",
    "\n",
    "plot_one_image(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но если вы хотите найти дефекты выпуклости, вам нужно передать returnPoints = False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка выпуклости\n",
    "Есть функция, чтобы проверить, является ли кривая выпуклой или нет, **cv2.isContourConvex()**. Это просто возвращает True или False\n",
    "\n",
    "Подумайте, как это можно сделать без этой функции?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:24:33.857769Z",
     "start_time": "2020-10-17T18:24:33.843581Z"
    }
   },
   "outputs": [],
   "source": [
    "k = cv2.isContourConvex(cnt)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ограничивающий прямоугольник\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прямой ограничивающий прямоугольник\n",
    "Это прямой прямоугольник, он не учитывает вращение объекта. Таким образом, площадь ограничивающего прямоугольника не будет минимальной. Он находится функцией **cv2.boundingRect()**.\n",
    "\n",
    "Пусть $(x,y)$ $-$ верхняя левая координата прямоугольника, а $(w,h)$ $-$ его ширина и высота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:24:51.956965Z",
     "start_time": "2020-10-17T18:24:51.790928Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "cv2.rectangle(img2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "plot_one_image(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Повернутый прямоугольник\n",
    "Здесь ограничивающий прямоугольник рисуется с минимальной площадью, поэтому он учитывает и вращение. Используемая функция $-$ **cv2.minAreaRect()**. Он возвращает структуру **Box2D**, которая содержит следующие детали $-$ (центр $(x, y)$, (ширина, высота), угол поворота). Но чтобы нарисовать этот прямоугольник, нам нужно $4$ угла прямоугольника. Получается функцией **cv2.boxPoints()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:25:07.553220Z",
     "start_time": "2020-10-17T18:25:07.388412Z"
    }
   },
   "outputs": [],
   "source": [
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "\n",
    "rect = cv2.minAreaRect(cnt)\n",
    "box = cv2.boxPoints(rect)\n",
    "box = np.int0(box)\n",
    "cv2.drawContours(img2, [box], 0, (0, 0, 255), 2)\n",
    "\n",
    "plot_one_image(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подгонка линии\n",
    "Точно так же мы можем подогнать линию к набору точек. Ниже изображение содержит набор белых точек. Мы можем приблизить к нему прямую линию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:25:15.413000Z",
     "start_time": "2020-10-17T18:25:15.231961Z"
    }
   },
   "outputs": [],
   "source": [
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "rows, cols = img.shape[:2]\n",
    "vx, vy, x, y = cv2.fitLine(cnt, cv2.DIST_L2, 0, 0.01, 0.01)\n",
    "lefty = int((-x * vy/ vx) + y)\n",
    "righty = int(((cols - x) * vy / vx) + y)\n",
    "cv2.line(img2, (cols-1, righty), (0, lefty), (0, 255, 0), 2)\n",
    "\n",
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "cv2.rectangle(img2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "plot_one_image(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свойства контура"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соотношение сторон\n",
    "Это отношение ширины к высоте ограничивающего прямоугольника объекта.\n",
    "\n",
    "${AspectRatio = \\frac{Width}{Height}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:25:38.128811Z",
     "start_time": "2020-10-17T18:25:38.122807Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "aspect_ratio = float(w) / h\n",
    "\n",
    "print(aspect_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Степень\n",
    "Степень $-$ это отношение площади контура к площади ограничивающего прямоугольника.\n",
    "\n",
    "${Extent=\\frac{Object\\ Area}{Bounding\\ Rectangle\\ Area}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:25:49.404920Z",
     "start_time": "2020-10-17T18:25:49.398915Z"
    }
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "x, y, w, h = cv2.boundingRect(cnt)\n",
    "rect_area = w * h\n",
    "extent = float(area) / rect_area\n",
    "\n",
    "print(extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solidity\n",
    "\n",
    "Solidity $-$ это отношение площади контура к его площади выпуклой оболочки.\n",
    "\n",
    "$Solidity = \\frac{Contour \\ Area}{Convex \\ Hull \\ Area}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:26:24.974894Z",
     "start_time": "2020-10-17T18:26:24.970875Z"
    }
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "hull = cv2.convexHull(cnt)\n",
    "hull_area = cv2.contourArea(hull)\n",
    "solidity = float(area) / hull_area\n",
    "\n",
    "print(solidity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эквивалентный диаметр\n",
    "\n",
    "Эквивалентный диаметр $-$ это диаметр круга, площадь которого равна площади контура.\n",
    "\n",
    "$Equivalent \\ Diameter = \\sqrt{\\frac{4 \\cdot \\ Contour \\ Area}{\\pi}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:26:38.361018Z",
     "start_time": "2020-10-17T18:26:38.356015Z"
    }
   },
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "equi_diameter = np.sqrt(4 * area / np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ориентация\n",
    "Ориентация $-$ это угол, под которым направлен объект. Следующий метод также дает длины **Major Axis** и **Minor Axis**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T13:15:55.386217Z",
     "start_time": "2020-10-17T13:15:55.379182Z"
    }
   },
   "outputs": [],
   "source": [
    "(x, y), (MA, ma), angle = cv2.fitEllipse(cnt)\n",
    "\n",
    "print(x, y, MA, ma, angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Маска и пиксельные точки\n",
    "В некоторых случаях нам могут понадобиться все точки, которые составляют этот объект. Это можно сделать следующим образом:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:29:20.231780Z",
     "start_time": "2020-10-17T18:29:20.214874Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = np.zeros(gray_img.shape, np.uint8)\n",
    "cv2.drawContours(mask, [cnt], 0, 255, -1)\n",
    "pixelpoints = np.transpose(np.nonzero(mask))\n",
    "print(f'Numpy shape: {pixelpoints.shape}')\n",
    "\n",
    "pixelpoints = cv2.findNonZero(mask)\n",
    "print(f'CV2 shape: {pixelpoints.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь два метода, один из которых использует функции Numpy, а другой $-$ функцию OpenCV (последняя закомментированная строка), дают то же самое. Результаты тоже такие же, но с небольшой разницей. Numpy дает координаты в формате **(строка, столбец)**, а OpenCV - в формате **(x, y)**. Так что в основном ответы будут взаимозаменяемы. Обратите внимание, что row = x и column = y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Максимальное значение, минимальное значение и их местоположение\n",
    "Мы можем найти эти параметры, используя изображение маски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:29:23.912343Z",
     "start_time": "2020-10-17T18:29:23.898558Z"
    }
   },
   "outputs": [],
   "source": [
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(gray_img, mask = mask)\n",
    "\n",
    "print(min_val, max_val, min_loc, max_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средний цвет или средняя интенсивность\n",
    "Здесь мы можем найти средний цвет объекта. Или это может быть средняя интенсивность объекта в режиме градаций серого. Мы снова используем ту же маску, чтобы сделать это.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:29:26.241399Z",
     "start_time": "2020-10-17T18:29:26.237398Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_val = cv2.mean(gray_img, mask = mask)\n",
    "\n",
    "print(mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Экстремальные точки\n",
    "Экстремальные точки означают самые верхние, самые нижние, самые правые и самые левые точки объекта.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:29:27.595095Z",
     "start_time": "2020-10-17T18:29:27.589089Z"
    }
   },
   "outputs": [],
   "source": [
    "leftmost = tuple(cnt[cnt[:, :, 0].argmin()][0])\n",
    "rightmost = tuple(cnt[cnt[:, :, 0].argmax()][0])\n",
    "topmost = tuple(cnt[cnt[:, :, 1].argmin()][0])\n",
    "bottommost = tuple(cnt[cnt[:, :, 1].argmax()][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T18:29:28.307680Z",
     "start_time": "2020-10-17T18:29:27.974091Z"
    }
   },
   "outputs": [],
   "source": [
    "print(leftmost, rightmost,'\\n', topmost, bottommost)\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.scatter(leftmost[0], leftmost[1])\n",
    "plt.scatter(rightmost[0], rightmost[1])\n",
    "plt.scatter(topmost[0], topmost[1])\n",
    "plt.scatter(bottommost[0], bottommost[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика - совместить 2 изображения (Image registration)\n",
    "\n",
    "Задача состоит в том, чтобы совместить друг с другом (преобразовать пиксель в пиксель) 2 изображения в оптическом и радиодиапазонах.\n",
    "\n",
    "Исходные данные доступны по [ссылке](https://disk.yandex.ru/d/AXvmkQRWrdStsQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
